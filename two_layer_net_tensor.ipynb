{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_layer_net_tensor.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO2nwLR6ahFCzvdMReNYdPx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pphos/pytorch_tutorial/blob/master/two_layer_net_tensor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAeqmouhgcJy",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch Tensors\n",
        "\n",
        "Numpyは優れたフレームワークですが, GPUを利用して数値計算を高速化することはできません.\n",
        "最新のディープニューラルネットワークの場合, GPUを用いることでCPUより50倍以上の高速化がはかれます.\n",
        "そのため, 最新のディープニューラルネットワークにはnumpyでは不十分です.\n",
        "\n",
        "ここでは, Pytorchの最も基本的な概念であるTensorを紹介します.\n",
        "Pytorch Tensorは概念的にはnumpy配列と同じです.\n",
        "Tensorはn次元配列であり, PytorchはこれらのTensorを操作するための多くの関数を提供します.\n",
        "Tensorは背後で計算グラフと勾配を追跡できるだけでなく,\n",
        "科学計算用の汎用ツールとしても役立ちます.\n",
        "\n",
        "また, Numpyとは異なり, Pytorch TensorはGPUを利用して数値計算を高速化できます. \n",
        "GPUでPytorch Tensorを実行するには, それを新しいデータ型にキャストするだけです.\n",
        "\n",
        "ここでは, Pytorch Tensorを使用して, 2層ネットワークをランダムデータに適合させます. \n",
        "numpyの例のように, ネットワークを介して順伝搬と逆伝搬を手動で実装する必要があります.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Vt8D6hikCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "331ffe55-e449-422a-84dd-a8f8977f22c1"
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\")\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y\n",
        "  h = x.mm(w1)\n",
        "  h_relu = h.clamp(min=0)\n",
        "  y_pred = h_relu.mm(w2)\n",
        "\n",
        "  # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum().item()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss)\n",
        "\n",
        "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "  grad_h = grad_h_relu.clone()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "  # Update weights using gradient descent\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 341.6982421875\n",
            "199 1.1551311016082764\n",
            "299 0.007652034051716328\n",
            "399 0.00020981239504180849\n",
            "499 3.932917752536014e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}