{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_layer_net_autograd.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjAsfW2Hed3I5H2bl07IIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pphos/pytorch_tutorial/blob/master/two_layer_net_autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryqx3x6lmJYC",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch: Tensors and autograd\n",
        "手動で逆伝搬を実装することは, 小規模な2層ネットワークでは大したことではありませんが,\n",
        "大規模で複雑なネットワークでは非常に困難になります.\n",
        "\n",
        "ありがたいことに, ニューラルネットワークの逆伝搬の計算を自動化するために, Pytorchでは自動微分パッケージ(autograd)が利用できます.\n",
        "autogradを使うと, ネットワークの順伝搬は計算グラフを定義します.\n",
        "グラフのノードはTensorであり, エッジは入力Tensorから出力Tensorを生成する関数です.\n",
        "このグラフを出力から入力に向かって辿っていくことで, 勾配を簡単に計算することができます.\n",
        "\n",
        "このことは複雑に見えますが, 実際に利用するのは非常に簡単です.\n",
        "各Tensorは, 計算グラフのノードを表します.\n",
        "`x`が`x.requires_grad = True`をもつTensorである場合,\n",
        "`x.grad`は, あるスカラー値に対する`x`の勾配を保持する別のTensorです.\n",
        "\n",
        "ここでは, Pytorch Tensorsとautogradを使って2層ネットワークを実装します.\n",
        "これで, ネットワークを介して逆伝搬を手動で実装する必要がなくなります.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuPtORK6ovPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "12f6f6e1-88be-4156-cb72-444f321420d8"
      },
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out, is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Create random Tensors for weights\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # Forward pass: compute predicted y using operations on Tensors;\n",
        "  # these are exactly the same operations we used to compute the forward pass\n",
        "  # using Tensors, but we do not need to keep references to intermidiate values\n",
        "  # since we are not implementing the backward pass by hand.\n",
        "  y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "  # Compute and print loss using operations on Tensors.\n",
        "  # Now loss is a Tensor of shape (1,)\n",
        "  # loss.item() gets the scalar value held in the loss.\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss.item())\n",
        "\n",
        "  # Use autograd to compute the backward pass. This call will compute the\n",
        "  # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "  # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
        "  # of the loss with respect to w1 and w2 respectively\n",
        "  loss.backward()\n",
        "\n",
        "  # Manually update weights using gradinet descent. Wrap in torch.no_grad()\n",
        "  # because weights have requires_grad=True, but we don't need to track this\n",
        "  # in autograd.\n",
        "  # An alternative way is to operate on weight.data and weight.grad.data.\n",
        "  # Recall that tensor.data gives a tensor that shares the storage with tensor,\n",
        "  # but dosen't track history.\n",
        "  # You can alse use torch.optim.SGD to achieve this.\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after updating weights\n",
        "    w1.grad.zero_()\n",
        "    w2.grad.zero_()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 1054.7764892578125\n",
            "199 13.261177062988281\n",
            "299 0.24380940198898315\n",
            "399 0.005378917790949345\n",
            "499 0.0003115542058367282\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}