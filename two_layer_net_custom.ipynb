{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "two_layer_net_custom.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOENIyC2na94OBWHJE0g/Eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pphos/pytorch_tutorial/blob/master/two_layer_net_custom.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JgDctarrl2j",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch: Defining new autograd functions\n",
        "内部では, それぞれのプリミティブautograd演算子は, 実際にはTensorで動作する2つの関数です.\n",
        "`forward`関数は入力テンソルから出力テンソルを計算します.\n",
        "`backward`関数はスカラー値に関する出力テンソルの勾配を受け取り, その同じスカラー値に関する入力テンソルの勾配を計算します.\n",
        "\n",
        "Pytorchでは, `torch.autograd.Function`を継承したクラスを定義し, `forward`関数と`backward`関数を実装することで, 独自の`autograd`演算子を簡単に定義できます.\n",
        "次に, インスタンスを作成して関数のように呼び出し, 入力データを含むTensorを渡すことで, 新しいautogradオペレータを使用できます.\n",
        "\n",
        "例では, ReLU非線形性を実行するための独自のautograd関数を定義し, それを使用して2層ネットワークを実装します.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhzYAGjxoXWb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "67aa4e5a-f203-4fa2-aff9-2afb4128cbc6"
      },
      "source": [
        "import torch\n",
        "\n",
        "class MyReLU(torch.autograd.Function):\n",
        "  \"\"\"\n",
        "  We can implement our custom autograd Functions by subclassing\n",
        "  torch.autograd.Function and implementing the forward and backward passes\n",
        "  which operate on Tensors.\n",
        "  \"\"\"\n",
        "\n",
        "  @staticmethod\n",
        "  def forward(ctx, input):\n",
        "    \"\"\"\n",
        "    In the forward pass we receive a Tensor containing the input and return\n",
        "    a Tensor containing the output. ctx is a context object that can be used\n",
        "    to stash information for backward computation. You can cache arbitrary\n",
        "    objects for use in the backward pass using the ctx.save_for_backward method.\n",
        "    \"\"\"\n",
        "    ctx.save_for_backward(input)\n",
        "    return input.clamp(min=0)\n",
        "\n",
        "  @staticmethod\n",
        "  def backward(ctx, grad_output):\n",
        "    \"\"\"\n",
        "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "    with respect to the output, and we need to compute the gradient of the loss\n",
        "    with respect to the input.\n",
        "    \"\"\"\n",
        "    input, = ctx.saved_tensors\n",
        "    grad_input = grad_output.clone()\n",
        "    grad_input[input < 0] = 0\n",
        "    return grad_input\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Create random Tensors for weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "  # To apply our Function, we use Function.apply method. We alias this as 'relu'\n",
        "  relu = MyReLU.apply\n",
        "\n",
        "  # Forward pass: compute predicted y using operations; we compute\n",
        "  # ReLU using our custom autograd operation.\n",
        "  y_pred = relu(x.mm(w1)).mm(w2)\n",
        "\n",
        "  # Compute and print loss\n",
        "  loss = (y_pred - y).pow(2).sum()\n",
        "  if t % 100 == 99:\n",
        "    print(t, loss.item())\n",
        "\n",
        "  # Use autograd to compute the backward pass.\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weights using gradient descent\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "\n",
        "    # Manually zero the gradients after updating weights\n",
        "    w1.grad.zero_();\n",
        "    w2.grad.zero_();"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "99 349.2152099609375\n",
            "199 1.0389724969863892\n",
            "299 0.004940866492688656\n",
            "399 0.00012938988220412284\n",
            "499 2.5338729756185785e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}